{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a47da0d0-0927-4adb-93e6-99a434f732cf",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2195672-0cab-4967-ba8a-c6544635547d",
   "metadata": {},
   "source": [
    "# Query analysis\n",
    "\n",
    "In any question answering application we need to search for, or retrieve, information based on a user question. In the simplest case, we can search on the user input directly. This approach has a few common failure modes:\n",
    "\n",
    "* The data has multiple attributes that a user input could be referring to,\n",
    "* The user input contains multiple distinct questions in it,\n",
    "* Search quality is sensitive to phrasing.\n",
    "\n",
    "To handle these, we can do **query analysis** to translate the raw user question into a query or queries optimized for our index. To illustrate, let's build a Q&A bot over the LangChain YouTube videos that performs query analysis.\n",
    "\n",
    "**NOTE**: This guide assumes familiarity with the basic building blocks of a simple RAG application outlined in the [Quickstart](/docs/use_cases/question_answering/quickstart)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4079b57-4369-49c9-b2ad-c809b5408d7e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168ef5c-e54e-49a6-8552-5502854a6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-community langchain-openai youtube-transcript-api pytube chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d66a45-a05c-4d22-b011-b1cdbdfc8f9c",
   "metadata": {},
   "source": [
    "### Set environment variables\n",
    "\n",
    "We'll use OpenAI in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "40e2979e-a818-4b96-ac25-039336f94319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b48b8-16d7-4089-bc17-f2d240b3935a",
   "metadata": {},
   "source": [
    "### Load documents\n",
    "\n",
    "We can use the `YouTubeLoader` to load transcripts of a few LangChain videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ae6921e1-3d5a-431c-9999-29a5f33201e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=pbAd8O1Lvm4\",\n",
    "    \"https://www.youtube.com/watch?v=ylrew7qb8sQ\",\n",
    "    \"https://www.youtube.com/watch?v=uRya4zRrRx4\",\n",
    "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
    "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\",\n",
    "    \"https://www.youtube.com/watch?v=3wAON0Lqviw\",\n",
    "    \"https://www.youtube.com/watch?v=jx7xuHlfsEQ\",\n",
    "    \"https://www.youtube.com/watch?v=xn1jEjRyJ2U\",\n",
    "    \"https://www.youtube.com/watch?v=SaDzIVkYqyY\",\n",
    "    \"https://www.youtube.com/watch?v=gqhlqdawHT4\",\n",
    "    \"https://www.youtube.com/watch?v=Ce03oEotdPs\",\n",
    "    \"https://www.youtube.com/watch?v=rZus0JtRqXE\",\n",
    "    \"https://www.youtube.com/watch?v=HAn9vnJy6S4\",\n",
    "    \"https://www.youtube.com/watch?v=dA1cHGACXCo\",\n",
    "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\",\n",
    "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
    "    \"https://www.youtube.com/watch?v=EhlPDL4QrWY\",\n",
    "    \"https://www.youtube.com/watch?v=mmBo8nlu2j0\",\n",
    "    \"https://www.youtube.com/watch?v=rQdibOsL1ps\",\n",
    "    \"https://www.youtube.com/watch?v=28lC4fqukoc\",\n",
    "    \"https://www.youtube.com/watch?v=es-9MgxB-uc\",\n",
    "    \"https://www.youtube.com/watch?v=wLRHwKuKvOE\",\n",
    "    \"https://www.youtube.com/watch?v=ObIltMaRJvY\",\n",
    "    \"https://www.youtube.com/watch?v=DjuXACWYkkU\",\n",
    "    \"https://www.youtube.com/watch?v=o7C9ld6Ln-M\",\n",
    "]\n",
    "docs = []\n",
    "for url in urls:\n",
    "    docs.extend(YoutubeLoader.from_youtube_url(url, add_video_info=True).load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3e1a99ee-1078-4373-b80a-630af48bf94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WebVoyager',\n",
       " 'Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'LangGraph: Planning Agents',\n",
       " 'LangGraph: Multi-Agent Workflows',\n",
       " 'Streaming Events: Introducing a new `stream_events` method',\n",
       " 'LangSmith: In-Depth Platform Overview',\n",
       " 'LangSmith in 10 Minutes',\n",
       " 'RAG from scratch: Part 8 (Query Translation -- Step Back)',\n",
       " 'RAG from scratch: Part 9 (Query Translation -- HyDE)',\n",
       " 'RAG from scratch: Part 7 (Query Translation -- Decomposition)',\n",
       " 'LangChain Agents with Open Source Models!',\n",
       " 'Gemini + Google Retrieval Agent from a LangChain Template',\n",
       " 'OpenGPTs',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve',\n",
       " 'Streaming Events: Introducing a new `stream_events` method',\n",
       " 'LangGraph: Multi-Agent Workflows',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'Auto-Prompt Builder (with Hosted LangServe)',\n",
       " 'Build a Full Stack RAG App With TypeScript',\n",
       " 'Getting Started with Multi-Modal LLMs',\n",
       " 'SQL Research Assistant',\n",
       " 'Skeleton-of-Thought: Building a New Template from Scratch',\n",
       " 'Benchmarking RAG over LangChain Docs',\n",
       " 'Building a Research Assistant from Scratch',\n",
       " 'LangServe and LangChain Templates Webinar']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "845149b7-130e-4228-ac80-d0a9286ef1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi this is Lance from Lang chain I'm going to be talking about using Lang graph to build a diverse and sophisticated rag flows so just to set the stage the basic rag flow you can see here starts with a question retrieval of relevant documents from an index which are passed into the context window of an llm for generation of an answer grounded in the ret documents so that's kind of the basic outline and we can see it's like a very linear path um in practice though you often encounter a few differ\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c7748415-ddbf-4c55-a242-c28833c03caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'pbAd8O1Lvm4',\n",
       " 'title': 'Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'description': 'Unknown',\n",
       " 'view_count': 7202,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/pbAd8O1Lvm4/hq720.jpg',\n",
       " 'publish_date': '2024-02-07 00:00:00',\n",
       " 'length': 1058,\n",
       " 'author': 'LangChain'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561697c8-b848-4b12-847c-ab6a8e2d1ae6",
   "metadata": {},
   "source": [
    "We can see that along with a transcription each document also has a title, view count, publication date, and length.\n",
    "\n",
    "### Indexing documents\n",
    "\n",
    "We'll use a vector store to index our documents, and we'll chunk them first to make our retrievals more concise and precise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "83734980-ee66-4d03-acbf-20e81492008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000, chunk_overlap=500, add_start_index=True\n",
    ")\n",
    "chunked_docs = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    chunked_docs, embeddings, collection_name=\"langchain_youtube\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d8d0a-5c1b-46b0-862c-a4eccfd5ae3c",
   "metadata": {},
   "source": [
    "## Retrieval without query analysis\n",
    "\n",
    "We can perform similarity search on a user question directly to find chunks relevant to the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "09435e9b-57b4-41b1-b34a-449815bdfae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-reflective RAG with LangGraph: Self-RAG and CRAG\n",
      "hi this is Lance from Lang chain I'm going to be talking about using Lang graph to build a diverse and sophisticated rag flows so just to set the stage the basic rag flow you can see here starts with a question retrieval of relevant documents from an index which are passed into the context window of an llm for generation of an answer grounded in the ret documents so that's kind of the basic outline and we can see it's like a very linear path um in practice though you often encounter a few differ\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"how do I build a RAG agent\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79ef1b-7edd-4b68-98e5-c0e4c0dd02e6",
   "metadata": {},
   "source": [
    "This works pretty well! Our first result is quite relevant to the question.\n",
    "\n",
    "Now what if we remembered that there was a video series titled \"RAG from scratch\" and wanted to find that specifically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a54de6ad-6f9f-49ca-86fd-5fcf3d318161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-reflective RAG with LangGraph: Self-RAG and CRAG\n",
      "hi this is Lance from Lang chain I'm going to be talking about using Lang graph to build a diverse and sophisticated rag flows so just to set the stage the basic rag flow you can see here starts with a question retrieval of relevant documents from an index which are passed into the context window of an llm for generation of an answer grounded in the ret documents so that's kind of the basic outline and we can see it's like a very linear path um in practice though you often encounter a few differ\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\n",
    "    \"rag from scratch\",\n",
    ")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "58067b4c-847d-4ad6-be65-2ac8afb1cd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'OpenGPTs',\n",
       " 'LangServe and LangChain Templates Webinar']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[res.metadata[\"title\"] for res in search_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891e8f5-ef0c-4ec0-b25f-eda7a5350a85",
   "metadata": {},
   "source": [
    "Since we're only searching over the transcriptions, and not over titles, our search misses all of the relevant documents. \n",
    "\n",
    "What if we wanted to search for results from a specific time period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7adbfc11-ca01-4883-8978-e4f6e4a1d23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenGPTs\n",
      "2024-01-31 00:00:00\n",
      "it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good visibility into what is going on we can see here we can see the response that we got back from tavil um and then we can see um the response from the AI and so there's lots of dad jokes in here this is u\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"videos on RAG published in 2023\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].metadata[\"publish_date\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790e2db-3c6e-440b-b6e8-ebdd6600fda5",
   "metadata": {},
   "source": [
    "Our first result is from 2024, and not very relevant to the input. Since we're just searching against document contents, there's no way for the results to be filtered on any document attributes.\n",
    "\n",
    "What if we wanted to know about deploying a LangChain chain as a REST API?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d52288ab-b048-4aa2-bf91-8183e14a9709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve\n",
      " further and see um that the inputs to each of those kind of Lambda steps is going to be one of those documents and then we're outputting um like the highlights DL segment and then formatting that with our prompt template uh if you recall from when we were constructing it um so now we have kind of our uh fully constructed chain uh for our uh search enable chatbot with XF um and now let's convert that to Lang serve um so to do that we'll go back to vs code um and here we're going to um start with\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"chain as rest api\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[1500:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3996c-35d8-42d9-a92b-b8c546d35346",
   "metadata": {},
   "source": [
    "This brings up LangServe, the package for deploying chains as REST API's, as desired.\n",
    "\n",
    "But what if we added that we wanted a chain that made use of multi-modal models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5183436a-9236-4b09-8774-08186dadc4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Events: Introducing a new `stream_events` method\n",
      "streaming is uh an incredibly important ux consideration for building L Ms in a few ways first of all even if you're just working with a single llm call it can often take a while and you might want to stream individual tokens to the user so they can see what's happening as the llm responds second of all a lot of the things that we build in the laying chain are more complicated chains or agents and so being able to stream the intermediate steps what tool are being called what the input to those t\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\n",
    "    \"chain as rest API, using multi-modal models\"\n",
    ")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c362d2-ec2d-4e0f-a122-130e809229cc",
   "metadata": {},
   "source": [
    "Our first result ends up not being about LangServe or multi-modal models. In reality \"chains as rest API\" and \"using multi-modal models\" are two fairly distinct questions that should be queried for separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57396e23-c192-4d97-846b-5eacea4d6b8d",
   "metadata": {},
   "source": [
    "## Query analysis\n",
    "\n",
    "To handle these failure modes we can perform **query analysis**. Specifically, we can perform:\n",
    "\n",
    "* **Query structuring**: If our documents have multiple searchable/filterable attributes, we can infer from any raw user question which specific attributes should be searched/filtered over. For example, when a user input specific something about video publication date, that should become a filter on the `publish_date` attribute of each document.\n",
    "* **Query decomposition**: If a user input contains multiple distinct questions, we can decompose the input into separate queries.\n",
    "* **Query expansion**: If an index is sensitive to query phrasing, we can multiple paraphrased versions of the user question to increase our chances of retrieving a relevant result.\n",
    "* **Query routing**: If there are multiple indexes that a question could be referring to, we can route each user question to the correct index.\n",
    "\n",
    "To do this we'll define a query schema and use a function-calling model to convert a user question into a structured query or queries. The structured nature of the query schema allows us to do query structuring and routing, and the fact that we can extract multiple of these \n",
    "allows us to do decomposition and expansion.\n",
    "\n",
    "### Query schema\n",
    "In this case we'll have explicit min and max attributes for view count, publication date, and video length so that those can be filtered on. And we'll add separate attributes for searches against the transcript contents versus the video title. We'll also add some sorting attributes that we'll touch on later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b51dd76-820d-41a4-98c8-893f6fe0d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Literal, Optional, Tuple\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class LangChainYouTubeSearch(BaseModel):\n",
    "    \"\"\"A query against a database of transcripts of LangChain YouTube videos.\"\"\"\n",
    "\n",
    "    content: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Similarity search query against the transcript of the video. Should specify EITHER content or title query but not both.\",\n",
    "    )\n",
    "    title: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Similarity search query against the title of the video. Should specify EITHER title or content query but not both.\",\n",
    "    )\n",
    "    min_view_count: Optional[int] = Field(\n",
    "        None, description=\"Minimum view count filter, inclusive.\"\n",
    "    )\n",
    "    max_view_count: Optional[int] = Field(\n",
    "        None, description=\"Maximum view count filter, exclusive.\"\n",
    "    )\n",
    "    earliest_publish_date: Optional[datetime.date] = Field(\n",
    "        None, description=\"Earliest publish date filter, inclusive.\"\n",
    "    )\n",
    "    latest_publish_date: Optional[datetime.date] = Field(\n",
    "        None, description=\"Latest publish date filter, exclusive.\"\n",
    "    )\n",
    "    min_length_sec: Optional[int] = Field(\n",
    "        None, description=\"Minimum video length in seconds, inclusive.\"\n",
    "    )\n",
    "    max_length_sec: Optional[int] = Field(\n",
    "        None, description=\"Maximum video length in seconds, exclusive.\"\n",
    "    )\n",
    "    sort_by: Literal[\n",
    "        \"relevance\",\n",
    "        \"view_count\",\n",
    "        \"publish_date\",\n",
    "        \"length\",\n",
    "    ] = Field(\"relevance\", description=\"Attribute to sort by\")\n",
    "    sort_order: Literal[\"ascending\", \"descending\"] = Field(\n",
    "        \"descending\", description=\"Whether to sort in ascending or descending order\"\n",
    "    )\n",
    "    relevance_rank: int = Field(\n",
    "        ...,\n",
    "        description=\"The index of this search query when all generated queries are sorted by the expected relevance of their results to the original user question. Each query must have a distinct index. A lower rank indicates higher relevance to the user question.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b08c52-1ce9-4d8b-a779-cbe8efde51d1",
   "metadata": {},
   "source": [
    "### Query generation\n",
    "\n",
    "To convert user questions to structured queries we'll make use of OpenAI's function-calling API. Since the latest OpenAI models can return multiple function invocations each turn, this approach automatically supports query expansion and decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "783c03c3-8c72-4f88-9cf4-5829ce6745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You're an expert at converting human questions into database queries. \\\n",
    "Given a question, return a list of database queries that will return relevant results. \\\n",
    "Include queries that are very specific to the user question and some that are more generic, \\\n",
    "in case the initial queries don't return any results. Think carefully about which fields \\\n",
    "should be specified in the same query and which should be specified in separate queries.\n",
    "\n",
    "Make sure to EXPAND queries in case there are synonyms or paraphrasings that might better match the database contents. \\\n",
    "This means returning multiple paraphrased versions of queries.\n",
    "\n",
    "Make sure to DECOMPOSE queries if there are multiple distinct questions within a single user input. \\\n",
    "This means returning each distinct question in the user input as its own query.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(\"examples\", optional=True),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([LangChainYouTubeSearch])\n",
    "query_analyzer = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | PydanticToolsParser(tools=[LangChainYouTubeSearch])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403517a-b8e3-44ac-b0a6-02f8305635a2",
   "metadata": {},
   "source": [
    "Let's see what queries our analyzer generates for the questions we searched earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "92bc7bac-700d-4666-b523-f0f8c3644ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='RAG from scratch' title=None min_view_count=None max_view_count=None earliest_publish_date=None latest_publish_date=None min_length_sec=None max_length_sec=None sort_by='relevance' sort_order='descending' relevance_rank=1\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke(\"rag from scratch\"):\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "af62af17-4f90-4dbd-a8b4-dfff51f1db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='RAG' title=None min_view_count=None max_view_count=None earliest_publish_date=datetime.date(2023, 1, 1) latest_publish_date=datetime.date(2024, 1, 1) min_length_sec=None max_length_sec=None sort_by='publish_date' sort_order='descending' relevance_rank=1\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke(\"videos on RAG published in 2023\"):\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "87590c6d-edd7-4805-bf68-c906907f9291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain as REST API' title=None min_view_count=None max_view_count=None earliest_publish_date=None latest_publish_date=None min_length_sec=None max_length_sec=None sort_by='relevance' sort_order='descending' relevance_rank=1\n",
      "\n",
      "content='multi-modal models with LangChain' title=None min_view_count=None max_view_count=None earliest_publish_date=None latest_publish_date=None min_length_sec=None max_length_sec=None sort_by='relevance' sort_order='descending' relevance_rank=2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke(\"chain as rest API, using multi-modal models\"):\n",
    "    print(query)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b519286-79f2-4490-8144-84e295563524",
   "metadata": {},
   "source": [
    "We can see that it correctly structures the time-based question to have a publication date filter, and decomposes the multi-part question into two separate questions. It still struggles with \"rag from scratch\" — namely, it doesn't search against video titles, but only against video contents. This likely because from the query schema alone it's not obvious when contents should be searched against and when titles should be searched against. To help with this, let's try adding some examples of question inputs and structured query outputs to our prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba81f0-f00b-4656-b840-037bf4306c60",
   "metadata": {},
   "source": [
    "## Adding examples\n",
    "\n",
    "Since we're working with OpenAI function-calling, we'll need to do a bit of extra structuring to send example inputs and outputs to the model. We'll create a `tool_example_to_messages` helper function to handle this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "80a33517-afa5-4152-a041-55e01eadf04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Dict, List\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "\n",
    "def tool_example_to_messages(example: Dict) -> List[BaseMessage]:\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    openai_tool_calls = []\n",
    "    for tool_call in example[\"tool_calls\"]:\n",
    "        openai_tool_calls.append(\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool_call.__class__.__name__,\n",
    "                    \"arguments\": tool_call.json(),\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    messages.append(\n",
    "        AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})\n",
    "    )\n",
    "    tool_outputs = example.get(\"tool_outputs\") or [\"...\"] * len(openai_tool_calls)\n",
    "    for output, tool_call in zip(tool_outputs, openai_tool_calls):\n",
    "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea54bde-7493-46e4-91a7-56d2f36d53e3",
   "metadata": {},
   "source": [
    "Now we can generate some examples of user inputs and the desired structured queries. We'll focus on examples that show how to route and expand queries, to either be against titles or content, how to structure them with filters, and how to decompose them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4d00d74b-7bc7-4224-ad09-fff8e7aeeaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "171f3c37-36da-4a80-911e-d0447168b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Web Voyager?\"\n",
    "queries = [\n",
    "    LangChainYouTubeSearch(title=\"Web Voyager\", relevance_rank=1),\n",
    "    LangChainYouTubeSearch(content=\"what is Web Voyager\", relevance_rank=2),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4099f07d-6033-4be6-8cb0-68b1c55d76bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How would I build a multi-agent system with LangGraph?\"\n",
    "queries = [\n",
    "    LangChainYouTubeSearch(\n",
    "        content=\"how to build a multi-agent system with LangGraph\", relevance_rank=1\n",
    "    ),\n",
    "    LangChainYouTubeSearch(\n",
    "        content=\"how to build a multi-agent system\", relevance_rank=2\n",
    "    ),\n",
    "    LangChainYouTubeSearch(title=\"LangGraph multi-agent\", relevance_rank=3),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "92523941-5aa0-4d1e-a795-1d14529b48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Have they released any chat langchain updates in 2024?\"\n",
    "queries = [\n",
    "    LangChainYouTubeSearch(\n",
    "        title=\"chat langchain\",\n",
    "        earliest_publish_date=datetime.date(2024, 1, 1),\n",
    "        latest_publish_date=datetime.date(2025, 1, 1),\n",
    "        relevance_rank=1,\n",
    "    ),\n",
    "    LangChainYouTubeSearch(\n",
    "        content=\"chat langchain\",\n",
    "        earliest_publish_date=datetime.date(2024, 1, 1),\n",
    "        latest_publish_date=datetime.date(2025, 1, 1),\n",
    "        relevance_rank=2,\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "844df58a-abd3-4c06-9a59-b7eccbbefc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I stream intermediate steps from a langgraph agent?\"\n",
    "queries = [\n",
    "    LangChainYouTubeSearch(\n",
    "        content=\"how to stream intermediate steps from LangGraph agent\",\n",
    "        relevance_rank=1,\n",
    "    ),\n",
    "    LangChainYouTubeSearch(\n",
    "        title=\"stream intermediate steps from LangGraph agent\",\n",
    "        relevance_rank=2,\n",
    "    ),\n",
    "    LangChainYouTubeSearch(content=\"how to stream with LangGraph\", relevance_rank=3),\n",
    "    LangChainYouTubeSearch(\n",
    "        content=\"how to stream intermediate steps\", relevance_rank=4\n",
    "    ),\n",
    "    LangChainYouTubeSearch(content=\"LangGraph agent stream\", relevance_rank=4),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c68fe800-75b8-44e8-aa10-7deffe33e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Overview of different approaches to RAG\"\n",
    "queries = [\n",
    "    LangChainYouTubeSearch(title=\"RAG overview\", relevance_rank=1),\n",
    "    LangChainYouTubeSearch(content=\"overview of RAG\", relevance_rank=2),\n",
    "    LangChainYouTubeSearch(content=\"different approaches to RAG\", relevance_rank=3),\n",
    "    LangChainYouTubeSearch(\n",
    "        content=\"overview of retrieval augmented generation\", relevance_rank=4\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "348fcd18-eaf5-490b-a3b4-f224ede066de",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_msgs = [msg for ex in examples for msg in _tool_example_to_messages(ex)]\n",
    "query_analyzer = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt.partial(examples=example_msgs)\n",
    "    | llm_with_tools\n",
    "    | PydanticToolsParser(tools=[LangChainYouTubeSearch])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "82824a2b-8985-430c-817a-6c8466bddf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=None title='RAG from scratch' min_view_count=None max_view_count=None earliest_publish_date=None latest_publish_date=None min_length_sec=None max_length_sec=None sort_by='relevance' sort_order='descending' relevance_rank=1\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke(\"rag from scratch\"):\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a13f6aac-90f2-4495-84c2-6ce9de53a8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LangChainYouTubeSearch(content='how to do extraction with Gemini using an agent', title=None, min_view_count=None, max_view_count=None, earliest_publish_date=None, latest_publish_date=None, min_length_sec=None, max_length_sec=None, sort_by='relevance', sort_order='descending', relevance_rank=1),\n",
       " LangChainYouTubeSearch(content='extraction with Gemini agent', title=None, min_view_count=None, max_view_count=None, earliest_publish_date=None, latest_publish_date=None, min_length_sec=None, max_length_sec=None, sort_by='relevance', sort_order='descending', relevance_rank=2),\n",
       " LangChainYouTubeSearch(content='Gemini agent extraction', title=None, min_view_count=None, max_view_count=None, earliest_publish_date=None, latest_publish_date=None, min_length_sec=None, max_length_sec=None, sort_by='relevance', sort_order='descending', relevance_rank=3)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\"How to do extraction with gemini using an agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f9dc0389-eb2f-40ca-a076-9b796b513fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LangChainYouTubeSearch(content='open GPTs', title=None, min_view_count=None, max_view_count=None, earliest_publish_date=None, latest_publish_date=None, min_length_sec=None, max_length_sec=None, sort_by='relevance', sort_order='descending', relevance_rank=1)]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\"what's open gpts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c65b2f-7881-45fc-a47b-a4eaaf48245f",
   "metadata": {},
   "source": [
    "## Retrieval with query analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc228c39-01f0-4475-b9bf-15d33033dbb7",
   "metadata": {},
   "source": [
    "### Sorting: Going beyond similarity search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "608f11cc-a24f-499a-941f-28f68cc16cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What has LangChain released lately?\"\n",
    "queries = [\n",
    "    LangChainYouTubeSearch(sort_by=\"publish_date\", relevance_rank=1),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9695291a-7f14-4eea-8a79-693bf3e0b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the most popular videos about RAG?\"\n",
    "queries = [\n",
    "    LangChainYouTubeSearch(title=\"RAG\", sort_by=\"view_count\", relevance_rank=1),\n",
    "    LangChainYouTubeSearch(content=\"RAG\", sort_by=\"view_count\", relevance_rank=2),\n",
    "    LangChainYouTubeSearch(\n",
    "        content=\"retrieval augmented generation\", sort_by=\"view_count\", relevance_rank=3\n",
    "    ),\n",
    "    LangChainYouTubeSearch(content=\"retrieval\", sort_by=\"view_count\", relevance_rank=4),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa52db91-622d-439b-bc2d-ed4b17c350eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are some short videos about agentic systems with local models?\"\n",
    "queries = [\n",
    "    LangChainYouTubeSearch(\n",
    "        content=\"how to build an agent using a local model\",\n",
    "        sort_by=\"length\",\n",
    "        sort_order=\"ascending\",\n",
    "        relevance_rank=1,\n",
    "    ),\n",
    "    LangChainYouTubeSearch(\n",
    "        title=\"agent local model\",\n",
    "        sort_by=\"length\",\n",
    "        sort_order=\"ascending\",\n",
    "        relevance_rank=2,\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5965eb3a-9dc1-4a01-9453-87f248de046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LangChainYouTubeSearch(content='local LLMs', title=None, min_view_count=None, max_view_count=None, earliest_publish_date=None, latest_publish_date=None, min_length_sec=None, max_length_sec=None, sort_by='publish_date', sort_order='descending', relevance_rank=1)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\"Recent talks about local llms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a5b80f99-5733-407a-a593-8f37868c4a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LangChainYouTubeSearch(content='new videos about streaming state machines', title=None, min_view_count=None, max_view_count=None, earliest_publish_date=None, latest_publish_date=None, min_length_sec=None, max_length_sec=None, sort_by='relevance', sort_order='descending', relevance_rank=1)]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\"new videos about streaming state machines\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-venv-2",
   "language": "python",
   "name": "poetry-venv-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
